{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This is a Bash cell\n",
    "wget -O - https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/city_market_tracker.tsv000.gz | aws s3 cp - s3://redfine-data-zone-7266/store_raw_data_yml/city_market_tracker.tsv000.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"RedfinDataAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redfin_data = spark.read.csv(\"s3://redfine-data-zone-7266/store_raw_data_yml/city_market_tracker.tsv000.gz\", header=True, inferSchema=True, sep= \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(redfin_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the schema\n",
    "redfin_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print column names\n",
    "redfin_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_redfin = redfin_data.select(['period_end','period_duration', 'city', 'state', 'property_type',\n",
    "    'median_sale_price', 'median_ppsf', 'homes_sold', 'inventory', 'months_of_supply', 'median_dom', 'sold_above_list', 'last_updated'])\n",
    "display(df_redfin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check total number of rows\n",
    "print(f\"Total number of rows: {df_redfin.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull\n",
    "# Count null values in each column\n",
    "null_counts = [df_redfin.where(isnull(col_name)).count() for col_name in df_redfin.columns]\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "for i, col_name in enumerate(df_redfin.columns):\n",
    "    print(f\"{col_name}: {null_counts[i]} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the entire DataFrame\n",
    "remaining_count = df_redfin.na.drop().count()\n",
    "\n",
    "print(f\"Number of missing rows: {df_redfin.count() - remaining_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of remaining rows: {remaining_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove na and count total number of remaining rows\n",
    "df_redfin = df_redfin.na.drop()\n",
    "print(f\"Total number of rows: {df_redfin.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column to confirm if we have removed all na\n",
    "null_counts = [df_redfin.where(isnull(col_name)).count() for col_name in df_redfin.columns]\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "#Extract year from period_end and save in a new column \"period_end_yr\"\n",
    "df_redfin = df_redfin.withColumn(\"period_end_yr\", year(col(\"period_end\")))\n",
    "\n",
    "#Extract month from period_end and save in a new column \"period_end_month\"\n",
    "df_redfin = df_redfin.withColumn(\"period_end_month\", month(col(\"period_end\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop period_end and last_updated columns\n",
    "df_redfin = df_redfin.drop(\"period_end\", \"last_updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_redfin3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "#let's map the month number to their respective month name.\n",
    "\n",
    "df_redfin = df_redfin.withColumn(\"period_end_month\", \n",
    "                   when(col(\"period_end_month\") == 1, \"January\")\n",
    "                   .when(col(\"period_end_month\") == 2, \"February\")\n",
    "                   .when(col(\"period_end_month\") == 3, \"March\")\n",
    "                   .when(col(\"period_end_month\") == 4, \"April\")\n",
    "                   .when(col(\"period_end_month\") == 5, \"May\")\n",
    "                   .when(col(\"period_end_month\") == 6, \"June\")\n",
    "                   .when(col(\"period_end_month\") == 7, \"July\")\n",
    "                   .when(col(\"period_end_month\") == 8, \"August\")\n",
    "                   .when(col(\"period_end_month\") == 9, \"September\")\n",
    "                   .when(col(\"period_end_month\") == 10, \"October\")\n",
    "                   .when(col(\"period_end_month\") == 11, \"November\")\n",
    "                   .when(col(\"period_end_month\") == 12, \"December\")\n",
    "                   .otherwise(\"Unknown\")\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_redfin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us write the final dataframe into our s3 bucket as a parquet file.\n",
    "s3_bucket = \"s3://redfine-data-zone-7266/redfine_transform_zone/redfin_data.parquet\"\n",
    "df_redfin.write.mode(\"overwrite\").parquet(s3_bucket)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
